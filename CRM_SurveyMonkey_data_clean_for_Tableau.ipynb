{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Summary of Code\n",
    "The code below was used to clean three csv files - a pre-program survey, a post-program survey, and normal CRM data pertaining to students that participated in programs offered by a fake study abroad provider - SAO. \n",
    "All data is masked as it originated from a real study abroad company. The code masks this data by creating fake studentid numbers, reduces the number of participants by a random percentage between 50% and 90%, and randomizes the cities in which they studied abroad. The data is cleaned and prepared for a left join in Tableau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import programming languages for data cleaning/manipulation, as well as exporting to an excel file with \n",
    "#multiple sheets for Tableau - I always import matplotlib in case I want to include data visualizations in \n",
    "#my notebooks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as plt\n",
    "import xlsxwriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import datatime function in order to label records and exported files with today's date\n",
    "import datetime as tdy     \n",
    "datestr = tdy.datetime.today\n",
    "tdy.datetime.today().strftime(\"%y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload first of three CSV files containing answers to surveys administered to clients during\n",
    "#three stages of their product cycle - pre, mid, and post. I needed to use different encoding option\n",
    "#because of the file format, most likely due to certain characters in the strings: ñ, í, é, á\n",
    "df_PPS = pd.read_csv('Post-Program Survey.csv', encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purposes of this demonstration, we are only going to keep NPS score questions, along with outcomes assessment\n",
    "#questions, related to skills gained during program participation, and specific learning outcomes questions, because\n",
    "#of this, we can drop a number of columns present in the original survey exported csv, using the .drop function,\n",
    "#starting with later columns-\n",
    "df_PPS.drop(df_PPS.columns[180:185], axis=1, inplace=True)\n",
    "df_PPS.drop(df_PPS.columns[68:165], axis=1, inplace=True)\n",
    "df_PPS.drop(df_PPS.columns[15:56], axis=1, inplace=True)\n",
    "df_PPS.drop(df_PPS.columns[14], axis=1, inplace=True)\n",
    "df_PPS.drop(df_PPS.columns[11:13], axis=1, inplace=True)\n",
    "df_PPS.drop(df_PPS.columns[10], axis=1, inplace=True)\n",
    "df_PPS.drop(df_PPS.columns[6:9], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column headers for Post Program Survey, this will make it easier to create visualizations in Tableau\n",
    "df_PPS.rename(columns={'Respondent ID':'respondent_ID', \n",
    "                       'Collector ID':'collector_ID',\n",
    "                      'Start Date':'resp_start_date',\n",
    "                       'End Date':'resp_end_date',\n",
    "                       'IP Address':'IP_address',\n",
    "                       'Email Address':'email',\n",
    "                       'Custom Data 2':'program_code',\n",
    "                       df_PPS.columns[7]:'Q1_would_rec',\n",
    "                       df_PPS.columns[8]:'Q8_a_PG_dev_new_skills',\n",
    "                       df_PPS.columns[9]:'Q8_b_PG_imprv_frgn_lang_skls',\n",
    "                       df_PPS.columns[10]:'Q8_c_PG_learn_culture_way_of_life',\n",
    "                       df_PPS.columns[11]:'Q8_d_PG_learn_history_host_country',\n",
    "                       df_PPS.columns[12]:'Q8_e_PG_expl_heritage',\n",
    "                       df_PPS.columns[13]:'Q8_f_PG_better_know_regional_polit_curnt_evnts_',\n",
    "                       df_PPS.columns[14]:'Q8_g_PG_learned_about_myself',\n",
    "                       df_PPS.columns[15]:'Q8_h_PG_earnd_credt_to_grad',\n",
    "                       df_PPS.columns[16]:'Q8_i_PG_gained_independ',\n",
    "                       df_PPS.columns[17]:'Q8_j_PG_expl_environ',\n",
    "                       df_PPS.columns[18]:'Q8_k_PG_positiv_contribut_host_commun',\n",
    "                       df_PPS.columns[19]:'Q8_l_PG_qual_other',\n",
    "                       df_PPS.columns[20]:'Q32_a_DM_ID_Q1',\n",
    "                       df_PPS.columns[21]:'Q32_b_DM_HI_Q1',\n",
    "                       df_PPS.columns[22]:'Q32_c_DM_SP_Q1',\n",
    "                       df_PPS.columns[23]:'Q32_d_DM_PF_Q1',\n",
    "                       df_PPS.columns[24]:'Q32_e_DM_EV_Q1',\n",
    "                       df_PPS.columns[25]:'Q33_a_DM_ID_Q2',\n",
    "                       df_PPS.columns[26]:'Q33_b_DM_HI_Q2',\n",
    "                       df_PPS.columns[27]:'Q33_c_DM_SP_Q2',\n",
    "                       df_PPS.columns[28]:'Q33_d_DM_PF_Q2',\n",
    "                       df_PPS.columns[29]:'Q33_e_DM_EV_Q2',\n",
    "                       df_PPS.columns[30]:'Q34_a_DM_ID_Q3',\n",
    "                       df_PPS.columns[31]:'Q34_b_DM_HI_Q3',\n",
    "                       df_PPS.columns[32]:'Q34_c_DM_SP_Q3',\n",
    "                       df_PPS.columns[33]:'Q34_d_DM_PF_Q3',\n",
    "                       df_PPS.columns[34]:'Q34_e_DM_EV_Q3',\n",
    "                      \n",
    "}\n",
    "                     , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check out the dataframe to make sure it is set up the way we want. We still need to change datatypes, we can\n",
    "#take care of that later, lets get the pre-departure survey sorted out now. \n",
    "df_PPS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading the pre-departure survey, again using sepecific encoding. \n",
    "df_PDS = pd.read_csv('Pre-Departure Survey.csv', encoding = \"ISO-8859-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purposes of this demonstration, we are only going to keep NPS score questions, along with outcomes assessment\n",
    "#questions, related to skills they hope to gain with program participation, and specific learning outcomes questions, because\n",
    "#of this, we can drop a number of columns present in the original survey exported csv, using the .drop function,\n",
    "#starting with later columns-\n",
    "df_PDS.drop(df_PDS.columns[109:114], axis=1, inplace = True)\n",
    "df_PDS.drop(df_PDS.columns[41:94], axis=1, inplace = True)\n",
    "df_PDS.drop(df_PDS.columns[15:29], axis=1, inplace=True)\n",
    "df_PDS.drop(df_PDS.columns[10:14], axis=1, inplace=True)\n",
    "df_PDS.drop(df_PDS.columns[6:9], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column headers for Pre-departure survey dataframe, again making it easier to sort data in Tableau\n",
    "df_PDS.rename(columns={'Respondent ID':'respondent_ID', \n",
    "                       'Collector ID':'collector_ID',\n",
    "                      'Start Date':'resp_start_date',\n",
    "                       'End Date':'resp_end_date',\n",
    "                       'IP Address':'IP_address',\n",
    "                       'Email Address':'email',\n",
    "                       'First Name':'first_name',\n",
    "                       'Last Name':'last_name',\n",
    "                       'Custom Data 2':'program_code',\n",
    "                       df_PDS.columns[7]:'Q1_would_rec',\n",
    "                       df_PDS.columns[8]:'Q5_a_PG_dev_new_skills',\n",
    "                       df_PDS.columns[9]:'Q5_b_PG_imprv_frgn_lang_skls',\n",
    "                       df_PDS.columns[10]:'Q5_c_PG_learn_culture_way_of_life',\n",
    "                       df_PDS.columns[11]:'Q5_d_PG_learn_history_host_country',\n",
    "                       df_PDS.columns[12]:'Q5_e_PG_expl_heritage',\n",
    "                       df_PDS.columns[13]:'Q5_f_PG_better_know_regional_polit_curnt_evnts_',\n",
    "                       df_PDS.columns[14]:'Q5_g_PG_learned_about_myself',\n",
    "                       df_PDS.columns[15]:'Q5_h_PG_earnd_credt_to_grad',\n",
    "                       df_PDS.columns[16]:'Q5_i_PG_gained_independ',\n",
    "                       df_PDS.columns[17]:'Q5_j_PG_expl_environ',\n",
    "                       df_PDS.columns[18]:'Q5_k_PG_positiv_contribut_host_commun',\n",
    "                       df_PDS.columns[19]:'Q5_l_PG_qual_other', \n",
    "                       df_PDS.columns[20]:'Q15_a_DM_ID_Q1',\n",
    "                       df_PDS.columns[21]:'Q15_b_DM_HI_Q1',\n",
    "                       df_PDS.columns[22]:'Q15_c_DM_SP_Q1',\n",
    "                       df_PDS.columns[23]:'Q15_d_DM_PF_Q1',\n",
    "                       df_PDS.columns[24]:'Q15_e_DM_EV_Q1',\n",
    "                       df_PDS.columns[25]:'Q16_a_DM_ID_Q2',\n",
    "                       df_PDS.columns[26]:'Q16_b_DM_HI_Q2',\n",
    "                       df_PDS.columns[27]:'Q16_c_DM_SP_Q2',\n",
    "                       df_PDS.columns[28]:'Q16_d_DM_PF_Q2',\n",
    "                       df_PDS.columns[29]:'Q16_e_DM_EV_Q2',\n",
    "                       df_PDS.columns[30]:'Q17_a_DM_ID_Q3',\n",
    "                       df_PDS.columns[31]:'Q17_b_DM_HI_Q3',\n",
    "                       df_PDS.columns[32]:'Q17_c_DM_SP_Q3',\n",
    "                       df_PDS.columns[33]:'Q17_d_DM_PF_Q3',\n",
    "                       df_PDS.columns[34]:'Q17_e_DM_EV_Q3',   \n",
    "                       \n",
    "}\n",
    "                     , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop second row of data for both surveys, surveymonkey exports csv files with two row headers\n",
    "df_PPS.drop([0], axis=0, inplace=True)\n",
    "df_PDS.drop([0], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets take a look at what is left of the pre-departure survey questions-\n",
    "df_PDS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will needed to convert PDS numeric columns to dates and ints using \"to_datetime\" and \"to_numeric\", that is because\n",
    "#when importing the file, these numeric columns were encoded as strings. \n",
    "df_PDS.resp_start_date = pd.to_datetime(df_PDS.resp_start_date)\n",
    "df_PDS.resp_end_date = pd.to_datetime(df_PDS.resp_end_date)\n",
    "df_PDS.Q1_would_rec = pd.to_numeric(df_PDS['Q1_would_rec'], errors='ignore')\n",
    "df_PDS.Q15_a_DM_ID_Q1 = pd.to_numeric(df_PDS['Q15_a_DM_ID_Q1'], errors='ignore')\n",
    "df_PDS.Q15_b_DM_HI_Q1 = pd.to_numeric(df_PDS['Q15_b_DM_HI_Q1'], errors='ignore')\n",
    "df_PDS.Q15_c_DM_SP_Q1 = pd.to_numeric(df_PDS['Q15_c_DM_SP_Q1'], errors='ignore')\n",
    "df_PDS.Q15_d_DM_PF_Q1 = pd.to_numeric(df_PDS['Q15_d_DM_PF_Q1'], errors='ignore')\n",
    "df_PDS.Q15_e_DM_EV_Q1 = pd.to_numeric(df_PDS['Q15_e_DM_EV_Q1'], errors='ignore')\n",
    "df_PDS.Q16_a_DM_ID_Q2 = pd.to_numeric(df_PDS['Q16_a_DM_ID_Q2'], errors='ignore')\n",
    "df_PDS.Q16_b_DM_HI_Q2 = pd.to_numeric(df_PDS['Q16_b_DM_HI_Q2'], errors='ignore')\n",
    "df_PDS.Q16_c_DM_SP_Q2 = pd.to_numeric(df_PDS['Q16_c_DM_SP_Q2'], errors='ignore')\n",
    "df_PDS.Q16_d_DM_PF_Q2 = pd.to_numeric(df_PDS['Q16_d_DM_PF_Q2'], errors='ignore')\n",
    "df_PDS.Q16_e_DM_EV_Q2 = pd.to_numeric(df_PDS['Q16_e_DM_EV_Q2'], errors='ignore')\n",
    "df_PDS.Q17_a_DM_ID_Q3 = pd.to_numeric(df_PDS['Q17_a_DM_ID_Q3'], errors='ignore')\n",
    "df_PDS.Q17_b_DM_HI_Q3 = pd.to_numeric(df_PDS['Q17_b_DM_HI_Q3'], errors='ignore')\n",
    "df_PDS.Q17_c_DM_SP_Q3 = pd.to_numeric(df_PDS['Q17_c_DM_SP_Q3'], errors='ignore')\n",
    "df_PDS.Q17_d_DM_PF_Q3 = pd.to_numeric(df_PDS['Q17_d_DM_PF_Q3'], errors='ignore')\n",
    "df_PDS.Q17_e_DM_EV_Q3 = pd.to_numeric(df_PDS['Q17_e_DM_EV_Q3'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out the same step for the PPS - convert numeric columns to dates and ints\n",
    "df_PPS.resp_start_date = pd.to_datetime(df_PPS.resp_start_date)\n",
    "df_PPS.resp_end_date = pd.to_datetime(df_PPS.resp_end_date)\n",
    "df_PPS.Q1_would_rec = pd.to_numeric(df_PPS['Q1_would_rec'], errors='ignore')\n",
    "df_PPS.Q32_a_DM_ID_Q1 = pd.to_numeric(df_PPS['Q32_a_DM_ID_Q1'], errors='ignore')\n",
    "df_PPS.Q32_b_DM_HI_Q1 = pd.to_numeric(df_PPS['Q32_b_DM_HI_Q1'], errors='ignore')\n",
    "df_PPS.Q32_c_DM_SP_Q1 = pd.to_numeric(df_PPS['Q32_c_DM_SP_Q1'], errors='ignore')\n",
    "df_PPS.Q32_d_DM_PF_Q1 = pd.to_numeric(df_PPS['Q32_d_DM_PF_Q1'], errors='ignore')\n",
    "df_PPS.Q32_e_DM_EV_Q1 = pd.to_numeric(df_PPS['Q32_e_DM_EV_Q1'], errors='ignore')\n",
    "df_PPS.Q33_a_DM_ID_Q2 = pd.to_numeric(df_PPS['Q33_a_DM_ID_Q2'], errors='ignore')\n",
    "df_PPS.Q33_b_DM_HI_Q2 = pd.to_numeric(df_PPS['Q33_b_DM_HI_Q2'], errors='ignore')\n",
    "df_PPS.Q33_c_DM_SP_Q2 = pd.to_numeric(df_PPS['Q33_c_DM_SP_Q2'], errors='ignore')\n",
    "df_PPS.Q33_d_DM_PF_Q2 = pd.to_numeric(df_PPS['Q33_d_DM_PF_Q2'], errors='ignore')\n",
    "df_PPS.Q33_e_DM_EV_Q2 = pd.to_numeric(df_PPS['Q33_e_DM_EV_Q2'], errors='ignore')\n",
    "df_PPS.Q34_a_DM_ID_Q3 = pd.to_numeric(df_PPS['Q34_a_DM_ID_Q3'], errors='ignore')\n",
    "df_PPS.Q34_b_DM_HI_Q3 = pd.to_numeric(df_PPS['Q34_b_DM_HI_Q3'], errors='ignore')\n",
    "df_PPS.Q34_c_DM_SP_Q3 = pd.to_numeric(df_PPS['Q34_c_DM_SP_Q3'], errors='ignore')\n",
    "df_PPS.Q34_d_DM_PF_Q3 = pd.to_numeric(df_PPS['Q34_d_DM_PF_Q3'], errors='ignore')\n",
    "df_PPS.Q34_e_DM_EV_Q3 = pd.to_numeric(df_PPS['Q34_e_DM_EV_Q3'], errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PPS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PDS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate rows from PPS, PDS as for some reason the administrators of the survey sent created multiple\n",
    "#collectors for the same set of student groups. This was done when we first switched protocol, from\n",
    "#weblink collectors links to email collectors. With data LEFT joined, any duplicates will multiply records and \n",
    "#skew numbers higher. \n",
    "\n",
    "df_PPS.drop_duplicates(subset=['email', 'program_code']\n",
    "                       , inplace=True)\n",
    "df_PDS.drop_duplicates(subset=['email', 'program_code']\n",
    "                       , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PPS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PDS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import data from our CRM, normally pulled and shaped using SQL, when connected to a normal database. For\n",
    "#our purposes, the data is present in a CSV file. It will be important to mask this data for public consumption\n",
    "#as this was taken from an office's (SAO) actual CRM. \n",
    "df_CRM = pd.read_csv('CRM_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to mask the SAO's personid, we are going to create a masked personid column by randomly generating numbers\n",
    "#into a new dataframe, based on the shape of the original personid column (length)\n",
    "#This is also why we imported the random function at the beginning of the notebook. \n",
    "\n",
    "data = np.random.randint(100000,999999,df_CRM.shape[0])\n",
    "df_rand = pd.DataFrame(data, columns=['maskPersonid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we join that new, random numbered column to the dataframe, and we will eventually drop the original personid \n",
    "#column\n",
    "numbers = df_rand['maskPersonid']\n",
    "df_CRM = df_CRM.join(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to mask the data even more, lets ramdomize the cities using a list of cities that students could potentially \n",
    "#travel to, then we need to give these city their appropriate countries so the Tableau map (using the next line of \n",
    "#code)function has the correct data to plot coordinates\n",
    "\n",
    "\n",
    "\n",
    "citieslist = [\"Newcastle\", \"Barcelona\",\"Salamanca\",\"Gold Coast\",\"Port Elizabeth\",\"Prague\",\"Dublin\",\"Granada\",\"Berlin\",\"Florence\",\"Seoul\",\"Athens\",\"Auckland\",\"Paris\",\"Christchurch\",\n",
    "\"Busan\",\"Madrid\",\"Galway\",\"Valpara√≠so and Vi√±a del Mar\",\"Barranquilla\",\"London\",\"Cape Town\",\"San Jos√©\",\"Wellington\",\"Melbourne\",\"Seville\",\"Cusco\",\"Canberra\",\"Shanghai\",\n",
    "\"Florian√≥polis\",\"Rome\",\"Reading\",\"Bangkok\",\"Santiago\",\"Lima\",\"Heredia\",\"Buenos Aires\",\"Sydney\",\"Bilbao\",\"Limerick\",\"Santander\",\"Stirling\",\"Adelaide\",\"Dunedin\",\"Glasgow\",\n",
    "\"Amsterdam\",\"Meknes\",\"Ho Chi Minh City\",\"M√°laga\",\"Lille\",\"Brisbane\",\"Milan\",\"Suva\",\"Townsville\",\"Palmerston North\",\"Medell√≠n\",\"Tokyo\",\"Oxford\",\"Cairns\",\"Perth\",\"Cartago\",\n",
    "\"Valencia\"]\n",
    "\n",
    "df_CRM['city'] = np.random.choice(citieslist, len(df_CRM), replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random cities to real countries using .loc-\n",
    "\n",
    "df_CRM.loc[df_CRM['city'] == 'Newcastle', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Barcelona', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Salamanca', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Gold Coast', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Port Elizabeth', 'Country'] = 'South Africa'\n",
    "df_CRM.loc[df_CRM['city'] == 'Prague', 'Country'] = 'Czech Republic'\n",
    "df_CRM.loc[df_CRM['city'] == 'Dublin', 'Country'] = 'Ireland'\n",
    "df_CRM.loc[df_CRM['city'] == 'Granada', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Berlin', 'Country'] = 'Germany'\n",
    "df_CRM.loc[df_CRM['city'] == 'Florence', 'Country'] = 'Italy'\n",
    "df_CRM.loc[df_CRM['city'] == 'Seoul', 'Country'] = 'South Korea'\n",
    "df_CRM.loc[df_CRM['city'] == 'Athens', 'Country'] = 'Greece'\n",
    "df_CRM.loc[df_CRM['city'] == 'Auckland', 'Country'] = 'New Zealand'\n",
    "df_CRM.loc[df_CRM['city'] == 'Paris', 'Country'] = 'France'\n",
    "df_CRM.loc[df_CRM['city'] == 'Christchurch', 'Country'] = 'New Zealand'\n",
    "df_CRM.loc[df_CRM['city'] == 'Busan', 'Country'] = 'South Korea'\n",
    "df_CRM.loc[df_CRM['city'] == 'Madrid', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Galway', 'Country'] = 'Ireland'\n",
    "df_CRM.loc[df_CRM['city'] == 'Valpara√≠so and Vi√±a del Mar', 'Country'] = 'Chile'\n",
    "df_CRM.loc[df_CRM['city'] == 'Barranquilla', 'Country'] = 'Colombia'\n",
    "df_CRM.loc[df_CRM['city'] == 'London', 'Country'] = 'England'\n",
    "df_CRM.loc[df_CRM['city'] == 'Cape Town', 'Country'] = 'South Africa'\n",
    "df_CRM.loc[df_CRM['city'] == 'San Jos√©', 'Country'] = 'Costa Rica'\n",
    "df_CRM.loc[df_CRM['city'] == 'Wellington', 'Country'] = 'New Zealand'\n",
    "df_CRM.loc[df_CRM['city'] == 'Seville', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Cusco', 'Country'] = 'Peru'\n",
    "df_CRM.loc[df_CRM['city'] == 'Canberra', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Shanghai', 'Country'] = 'China'\n",
    "df_CRM.loc[df_CRM['city'] == 'Florian√≥polis', 'Country'] = 'Brazil'\n",
    "df_CRM.loc[df_CRM['city'] == 'Rome', 'Country'] = 'Italy'\n",
    "df_CRM.loc[df_CRM['city'] == 'Reading', 'Country'] = 'England'\n",
    "df_CRM.loc[df_CRM['city'] == 'Bangkok', 'Country'] = 'Thailand'\n",
    "df_CRM.loc[df_CRM['city'] == 'Santiago', 'Country'] = 'Chile'\n",
    "df_CRM.loc[df_CRM['city'] == 'Lima', 'Country'] = 'Peru'\n",
    "df_CRM.loc[df_CRM['city'] == 'Heredia', 'Country'] = 'Costa Rica'\n",
    "df_CRM.loc[df_CRM['city'] == 'Buenos Aires', 'Country'] = 'Argentina'\n",
    "df_CRM.loc[df_CRM['city'] == 'Sydney', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Bilbao', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Limerick', 'Country'] = 'Ireland'\n",
    "df_CRM.loc[df_CRM['city'] == 'Stirling', 'Country'] = 'Scotland'\n",
    "df_CRM.loc[df_CRM['city'] == 'Adelaide', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Dunedin', 'Country'] = 'New Zealand'\n",
    "df_CRM.loc[df_CRM['city'] == 'Glasgow', 'Country'] = 'Scotland'\n",
    "df_CRM.loc[df_CRM['city'] == 'Amsterdam', 'Country'] = 'The Netherlands'\n",
    "df_CRM.loc[df_CRM['city'] == 'Meknes', 'Country'] = 'Morocco'\n",
    "df_CRM.loc[df_CRM['city'] == 'Ho Chi Minh City', 'Country'] = 'Vietnam'\n",
    "df_CRM.loc[df_CRM['city'] == 'M√°laga', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Lille', 'Country'] = 'France'\n",
    "df_CRM.loc[df_CRM['city'] == 'Brisbane', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Milan', 'Country'] = 'Italy'\n",
    "df_CRM.loc[df_CRM['city'] == 'Suva', 'Country'] = 'Fiji'\n",
    "df_CRM.loc[df_CRM['city'] == 'Townsville', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Palmerston North', 'Country'] = 'New Zealand'\n",
    "df_CRM.loc[df_CRM['city'] == 'Medell√≠n', 'Country'] = 'Colombia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Tokyo', 'Country'] = 'Japan'\n",
    "df_CRM.loc[df_CRM['city'] == 'Oxford', 'Country'] = 'England'\n",
    "df_CRM.loc[df_CRM['city'] == 'Perth', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Cartago', 'Country'] = 'Costa Rica'\n",
    "df_CRM.loc[df_CRM['city'] == 'Valencia', 'Country'] = 'Spain'\n",
    "df_CRM.loc[df_CRM['city'] == 'Melbourne', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Cairns', 'Country'] = 'Australia'\n",
    "df_CRM.loc[df_CRM['city'] == 'Santander', 'Country'] = 'Spain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take a random sample of this new, modified masked dataframe so we can better mask the data, this sample is \n",
    "#between 50% and 90% of the of the true dataset, we won't know which because we are using the random.uniform function-\n",
    "x = random.uniform(.5, .9)\n",
    "\n",
    "df_CRM = df_CRM.sample(frac=(x))\n",
    "\n",
    "\n",
    "#because we are going to be left joining the survey data to this set, we don't want to take a random fraction of the\n",
    "#survey data. We are going to be joining on email_address and program_code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets drop all of the columns we don't need, of course it would be interesting to analyze learning and satifaction\n",
    "#with referral type, method, and place of birth, etc.. but for this analysis lets keep it simple and only keep \n",
    "#the columns we are interested in. This CRM data was prevously cleaned using SQL - for example and GPA = 0 data \n",
    "#was converted to N/A, duplicates were removed, and records are based on session id, not package (account) or \n",
    "#unique person ID\n",
    "df_CRM.drop(df_CRM.columns[39:45], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[33:35], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[30:32], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[26], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[23], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[17:20], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[10:16], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[5:8], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[2:4], axis=1, inplace = True)\n",
    "df_CRM.drop(df_CRM.columns[0], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the start date and end date of programs to a datetime datatype \n",
    "df_CRM.enddate = pd.to_datetime(df_CRM.enddate)\n",
    "df_CRM.startdate = pd.to_datetime(df_CRM.startdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what our CRM data looks like now-\n",
    "df_CRM.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we also need to create a new business division, one not in our current CRM, based on the program_code, using \n",
    "#the first character in the code = i, and the 4th character = T or G\n",
    "df_CRM['SAOSP1'] = df_CRM['program_code'].str[0]\n",
    "df_CRM['SAOSP2'] = df_CRM['program_code'].str[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another dataframe containing records where all \n",
    "#rows with first letter in program code = I and forth letter \n",
    "#equals T or G, lets call this dataframe \"df_PN\"\n",
    "df_PN = df_CRM.loc[(df_CRM['SAOSP1'] == 'I') \n",
    "        & ((df_CRM['SAOSP2'] == 'T') | (df_CRM['SAOSP2'] == 'G'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This formula removes PN dataframe from our current CRM dataframe\n",
    "#just taking it out and creating a \"temporary\" subframe\n",
    "df_CRM = pd.concat([df_CRM, df_PN, df_PN]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the SAO business division \"bus div\" with the correct Business\n",
    "#division, using \"SP\" as the acronymn for the business division not currently in our CRM\n",
    "#remember we only do this to the df_PN \"temporary\" subframe-\n",
    "df_PN.bus_div.replace(to_replace=['SAO'], value='SP', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine df_PN database with df_CRM then drop both SAOSP1 and SAOSP2 columns (next line of code) \n",
    "#to bring our Dataframe back together\n",
    "df_CRM = pd.concat([df_CRM, df_PN], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code drops the two additional columns we created in order to make the additional business division\n",
    "df_CRM.drop(df_CRM.columns[21:23], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace out city names with correct names, removing symbols inherited from data pull/csv\n",
    "#this will ease transition into Tableau\n",
    "df_CRM.city.replace(to_replace=['M√°laga'],value='Málaga', inplace=True)\n",
    "df_CRM.city.replace(to_replace=['Florian√≥polis'],value='Florianópolis', inplace=True)\n",
    "df_CRM.city.replace(to_replace=['San Jos√©'],value='San José', inplace=True)\n",
    "df_CRM.city.replace(to_replace=['San Jos?'],value='San José', inplace=True)\n",
    "df_CRM.city.replace(to_replace=['Medell√≠n'],value='Medellín', inplace=True)\n",
    "df_CRM.city.replace(to_replace=['Valpara√≠so and Vi√±a del Mar'],value='Valparaíso and Viña del Mar', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets modify the dataframe to only include students who participated in programs in 2018-2019, and had their account \n",
    "#status as completed or currently abroad. \n",
    "df_CRM = df_CRM[df_CRM['account_status'].isin(['Completed', 'Currently Abroad']) ]\n",
    "df_CRM = df_CRM[df_CRM['year'].isin([2018,2019]) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that I have my dataframes created, this will allow me to combine all surveys and CRM data to same excel file,\n",
    "#and I can join my data in Tableau, using two fields - email_address and program_code\n",
    "LMS = 'LMS'+'.xlsx'\n",
    "LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives final excell document with all tabs in correct order, REMEMBER TO RENAME WITH TODAY'S DATE!\n",
    "writer = pd.ExcelWriter(LMS, engine='xlsxwriter')\n",
    "\n",
    "df_CRM.to_excel(writer, sheet_name='CRM', index=False)\n",
    "\n",
    "df_PDS.to_excel(writer, sheet_name='PDS', index=False)\n",
    "\n",
    "df_PPS.to_excel(writer, sheet_name='PPS', index=False)\n",
    "\n",
    "writer.save()\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
